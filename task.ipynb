{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice Lakes Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "- Use 'Run All' button of notebook to get final testing gpkg in 'GPKG' folder, or just run cells one by one.\n",
    "- If you need to adjust the batch size to match your GPU memory, change BATCH_SIZE below.\n",
    "- Training the unet costs for more than one day, and here we provide the model, if you need to train the unet by yourself, change IS_TRAIN. And this will change the old model and lake_polygons_testing.gpkg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IS_TRAIN = 0    #0 for just test and 1 for train with test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import fiona\n",
    "from osgeo import ogr\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "import unet\n",
    "import icelake\n",
    "\n",
    "np.seterr(invalid=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lake_polygons_training data division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_gpkg layers: ['lakes']\n",
      "\n",
      "splitting training_gpkg by image and region_num:\n",
      "[2 4 6 1 3 5]\n",
      "['Greenland26X_22W_Sentinel2_2019-06-03_05.tif'\n",
      " 'Greenland26X_22W_Sentinel2_2019-06-19_20.tif'\n",
      " 'Greenland26X_22W_Sentinel2_2019-07-31_25.tif'\n",
      " 'Greenland26X_22W_Sentinel2_2019-08-25_29.tif']\n",
      "Saved train_06_03_2\n",
      "Saved train_07_31_2\n",
      "Saved train_06_03_4\n",
      "Saved train_07_31_4\n",
      "Saved train_06_03_6\n",
      "Saved train_07_31_6\n",
      "Saved train_06_19_1\n",
      "Saved train_08_25_1\n",
      "Saved train_06_19_3\n",
      "Saved train_08_25_3\n",
      "Saved train_06_19_5\n",
      "Saved train_08_25_5\n",
      "done\n",
      "training_gpkg layers after split: ['lakes', 'train_06_03_2', 'train_07_31_2', 'train_06_03_4', 'train_07_31_4', 'train_06_03_6', 'train_07_31_6', 'train_06_19_1', 'train_08_25_1', 'train_06_19_3', 'train_08_25_3', 'train_06_19_5', 'train_08_25_5']\n"
     ]
    }
   ],
   "source": [
    "train_gpkg_path = \"data/lake_polygons_training.gpkg\"\n",
    "icelake.spilt_training_gpkg(train_gpkg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dem data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_index_zip_path = \"data/ArcticDEM_Mosaic_Index_v4_1_gpkg.zip\"\n",
    "if os.path.exists(dem_index_zip_path):\n",
    "    print('dem index zip already exists')\n",
    "else:\n",
    "    wget.download(\"https://data.pgc.umn.edu/elev/dem/setsm/ArcticDEM/indexes/ArcticDEM_Mosaic_Index_v4_1_gpkg.zip\", dem_index_zip_path)\n",
    "\n",
    "if not os.path.exists(\"data/ArcticDEM_Mosaic_Index_v4_1_gpkg.gpkg\"):\n",
    "    with zipfile.ZipFile(dem_index_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "else:\n",
    "    print('dem index gpkg already exists')\n",
    "os.remove(dem_index_zip_path)\n",
    "\n",
    "dem_folder = \"data/dem raster/\"\n",
    "if not os.path.exists(dem_folder):\n",
    "    os.mkdir(dem_folder)\n",
    "else:\n",
    "    print('dem raster folder already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 intersections\n",
      "downloading dem raster 15_40\n",
      "downloading dem raster 16_40\n",
      "downloading dem raster 14_39\n",
      "downloading dem raster 15_39\n",
      "downloading dem raster 16_39\n",
      "downloading dem raster 14_40\n",
      "downloading dem raster 15_38\n",
      "downloading dem raster 14_38\n",
      "downloading dem raster 16_38\n",
      "downloading dem raster 17_40\n",
      "downloading dem raster 17_38\n",
      "downloading dem raster 17_39\n",
      "downloading dem raster 18_39\n",
      "downloading dem raster 18_40\n",
      "downloading dem raster 13_39\n",
      "downloading dem raster 13_40\n",
      "downloading dem raster 13_38\n",
      "downloading dem raster 29_45\n",
      "downloading dem raster 29_46\n",
      "downloading dem raster 27_44\n",
      "downloading dem raster 29_44\n",
      "downloading dem raster 28_43\n",
      "downloading dem raster 27_45\n",
      "downloading dem raster 28_45\n",
      "downloading dem raster 29_43\n",
      "downloading dem raster 28_46\n",
      "downloading dem raster 28_44\n",
      "downloading dem raster 30_45\n",
      "downloading dem raster 31_44\n",
      "downloading dem raster 30_43\n",
      "downloading dem raster 31_45\n",
      "downloading dem raster 30_44\n"
     ]
    }
   ],
   "source": [
    "gdf_regions = gpd.read_file(\"data/lakes_regions.gpkg\", layer = \"regions\")\n",
    "gdf_dem_index = gpd.read_file(\"data/ArcticDEM_Mosaic_Index_v4_1_gpkg.gpkg\", layer = \"ArcticDEM_Mosaic_Index_v4_1_10m\")\n",
    "gdf_regions = gdf_regions.to_crs('EPSG:3413')\n",
    "intersect = gpd.sjoin(gdf_dem_index, gdf_regions, how=\"inner\", predicate=\"intersects\")\n",
    "intersect = intersect.drop(columns=[\"creationdate\", \"index_right\", \"region_num\"])\n",
    "intersect = intersect.drop_duplicates()\n",
    "print(len(intersect), \"intersections\")\n",
    "\n",
    "url_1 = \"https://pgc-opendata-dems.s3.us-west-2.amazonaws.com/arcticdem/mosaics/v4.1/10m/\"\n",
    "\n",
    "for index, row in intersect.iterrows():\n",
    "    url = url_1 + row['tile'] + \"/\" + row['tile'] + \"_10m_v4.1_dem.tif\"\n",
    "    path = dem_folder + row['tile'] + \"_dem.tif\"\n",
    "    if not os.path.exists(path):\n",
    "        wget.download(url, path)\n",
    "        print(\"downloading dem raster\", row['tile'])\n",
    "    else:\n",
    "        print(row['tile'], 'dem raster already exists')\n",
    "\n",
    "intersect = intersect.to_crs('EPSG:3857')\n",
    "intersect.to_file(\"data/lakes_regions.gpkg\", layer=\"dem_selected\", driver=\"GPKG\", append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### source raster division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 'Greenland26X_22W_Sentinel2_2019-06-03_05.tif' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-06-03_05.tif.aux.xml' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-06-19_20.tif' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-06-19_20.tif.aux.xml' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-07-31_25.tif' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-07-31_25.tif.aux.xml' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-08-25_29.tif' move successfully\n",
      "file 'Greenland26X_22W_Sentinel2_2019-08-25_29.tif.aux.xml' move successfully\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"data\"\n",
    "source_raster_folder = 'data/source raster'\n",
    "contents = os.listdir(folder_path)\n",
    "if not os.path.exists(source_raster_folder):\n",
    "    os.mkdir(source_raster_folder)\n",
    "else:\n",
    "    print('source raster folder already exists')\n",
    "\n",
    "for item in contents:\n",
    "    item_path = os.path.join(folder_path, item)\n",
    "    if 'Greenland' in item:\n",
    "        try:\n",
    "            shutil.move(os.path.join(folder_path, item), os.path.join(source_raster_folder))\n",
    "            print(f\"file '{item}' move successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06_03_1.tif done.\n",
      "06_03_2.tif done.\n",
      "06_03_3.tif done.\n",
      "06_03_4.tif done.\n",
      "06_03_5.tif done.\n",
      "06_03_6.tif done.\n",
      "06_19_1.tif done.\n",
      "06_19_2.tif done.\n",
      "06_19_3.tif done.\n",
      "06_19_4.tif done.\n",
      "06_19_5.tif done.\n",
      "06_19_6.tif done.\n",
      "07_31_1.tif done.\n",
      "07_31_2.tif done.\n",
      "07_31_3.tif done.\n",
      "07_31_4.tif done.\n",
      "07_31_5.tif done.\n",
      "07_31_6.tif done.\n",
      "08_25_1.tif done.\n",
      "08_25_2.tif done.\n",
      "08_25_3.tif done.\n",
      "08_25_4.tif done.\n",
      "08_25_5.tif done.\n",
      "08_25_6.tif done.\n"
     ]
    }
   ],
   "source": [
    "regions_gpkg_path = \"data/lakes_regions.gpkg\"\n",
    "origan_raster_path = [\n",
    "    \"data/source raster/Greenland26X_22W_Sentinel2_2019-06-03_05.tif\",\n",
    "    \"data/source raster/Greenland26X_22W_Sentinel2_2019-06-19_20.tif\",\n",
    "    \"data/source raster/Greenland26X_22W_Sentinel2_2019-07-31_25.tif\",\n",
    "    \"data/source raster/Greenland26X_22W_Sentinel2_2019-08-25_29.tif\",\n",
    "]\n",
    "splitted_raster_folder = \"data/splitted raster/\"\n",
    "if not os.path.exists(splitted_raster_folder):\n",
    "    os.mkdir(splitted_raster_folder)\n",
    "else:\n",
    "    print('splitted raster folder already exists')\n",
    "icelake.split_source_raster(\n",
    "    regions_gpkg_path, origan_raster_path, splitted_raster_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate NDWI-Ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06_03_1.tif NDWI_ice calculation and saving completed.\n",
      "06_03_2.tif NDWI_ice calculation and saving completed.\n",
      "06_03_3.tif NDWI_ice calculation and saving completed.\n",
      "06_03_4.tif NDWI_ice calculation and saving completed.\n",
      "06_03_5.tif NDWI_ice calculation and saving completed.\n",
      "06_03_6.tif NDWI_ice calculation and saving completed.\n",
      "06_19_1.tif NDWI_ice calculation and saving completed.\n",
      "06_19_2.tif NDWI_ice calculation and saving completed.\n",
      "06_19_3.tif NDWI_ice calculation and saving completed.\n",
      "06_19_4.tif NDWI_ice calculation and saving completed.\n",
      "06_19_5.tif NDWI_ice calculation and saving completed.\n",
      "06_19_6.tif NDWI_ice calculation and saving completed.\n",
      "07_31_1.tif NDWI_ice calculation and saving completed.\n",
      "07_31_2.tif NDWI_ice calculation and saving completed.\n",
      "07_31_3.tif NDWI_ice calculation and saving completed.\n",
      "07_31_4.tif NDWI_ice calculation and saving completed.\n",
      "07_31_5.tif NDWI_ice calculation and saving completed.\n",
      "07_31_6.tif NDWI_ice calculation and saving completed.\n",
      "08_25_1.tif NDWI_ice calculation and saving completed.\n",
      "08_25_2.tif NDWI_ice calculation and saving completed.\n",
      "08_25_3.tif NDWI_ice calculation and saving completed.\n",
      "08_25_4.tif NDWI_ice calculation and saving completed.\n",
      "08_25_5.tif NDWI_ice calculation and saving completed.\n",
      "08_25_6.tif NDWI_ice calculation and saving completed.\n"
     ]
    }
   ],
   "source": [
    "ndwi_ice_folder = \"data/ndwi ice/\"\n",
    "if not os.path.exists(ndwi_ice_folder):\n",
    "    os.mkdir(ndwi_ice_folder)\n",
    "else:\n",
    "    print('ndwi ice folder already exists')\n",
    "\n",
    "for dirname, _, filenames in os.walk(\"data/splitted raster/\"):\n",
    "    filenames.sort()\n",
    "    for filename in filenames:\n",
    "        raster_path = os.path.join(dirname, filename)\n",
    "        icelake.cal_ndwi(raster_path, ndwi_ice_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = fiona.listlayers(\"data/lake_polygons_training.gpkg\")\n",
    "if IS_TRAIN == 1:\n",
    "    train_tif, train_mask, val_tif, val_mask, lakes_true, tif_array = icelake.data_preprocess_train([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [], layers[1:], train_with_all_data=False)\n",
    "    print(f\"{train_tif.shape}\\t{train_mask.shape}\")\n",
    "    sliced_image_set = unet.run_unet(train_tif, train_mask, val_tif, val_mask, print_tqdm=False, BATCH_SIZE=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # val model, no need in testing\n",
    "# layers = fiona.listlayers(\"data/lake_polygons_training.gpkg\")\n",
    "# gdf_pred_all = gpd.GeoDataFrame()\n",
    "# kf = KFold(n_splits=len(layers[1:]), shuffle=True, random_state=42)\n",
    "# for train_layer_indices, val_layer_indices in kf.split(layers[1:]):\n",
    "#     tif_num = re.search(r\"train_(.+)\", layers[val_layer_indices[0] + 1]).group(1)\n",
    "\n",
    "#     train_tif, train_mask, val_tif, val_mask, lakes_true, tif_array = icelake.data_preprocess_train([], val_layer_indices, layers[1:], train_with_all_data=False)\n",
    "#     print(f\"Val {tif_num}: {val_tif.shape}\\t{val_mask.shape}\\t{lakes_true.shape}\")\n",
    "#     sliced_image_set = unet.run_unet(train_tif, train_mask, val_tif, val_mask, print_tqdm=False, BATCH_SIZE=BATCH_SIZE)\n",
    "#     lakes_pred = icelake.splice_image(lakes_true, sliced_image_set)\n",
    "#     del train_tif, train_mask, val_tif, val_mask, sliced_image_set\n",
    "\n",
    "#     # lakes_pred post process in tif\n",
    "#     tmp_mask = np.ones_like(lakes_pred)\n",
    "#     tmp_mask[np.all(tif_array == 0, axis=0)] = 0\n",
    "#     lakes_pred = lakes_pred & tmp_mask\n",
    "#     # lakes_pred post process in gpkg\n",
    "\n",
    "#     gdf_pred = icelake.array2gdf(lakes_pred, tif_num)\n",
    "#     del lakes_true, lakes_pred, tif_array, tmp_mask\n",
    "\n",
    "#     gdf_true = gpd.read_file(\n",
    "#         \"data/lake_polygons_training.gpkg\", layer=\"train_\" + tif_num\n",
    "#     )\n",
    "#     icelake.cal_scores_vector(gdf_true, gdf_pred)\n",
    "\n",
    "#     gdf_pred = icelake.post_process(gdf_pred, tif_num)\n",
    "#     gdf_pred[\"tif_num\"] = tif_num\n",
    "#     gdf_pred_all = pd.concat([gdf_pred_all, gdf_pred], ignore_index=True)\n",
    "    \n",
    "#     icelake.cal_scores_vector(gdf_true, gdf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"GPKG/lake_polygons_testing.gpkg\"):\n",
    "    gpkg_ds = ogr.GetDriverByName('GPKG').CreateDataSource(\"GPKG/lake_polygons_testing.gpkg\")\n",
    "    gpkg_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 06_03_1: (6936, 256, 256, 3)\t(6936, 256, 256)\t(3, 8731, 13076)\n",
      "=> Loading Checkpoint\n",
      "Test 06_03_3: (8400, 256, 256, 3)\t(8400, 256, 256)\t(3, 9721, 14412)\n",
      "=> Loading Checkpoint\n",
      "Test 06_03_5: (45684, 256, 256, 3)\t(45684, 256, 256)\t(3, 24192, 31214)\n",
      "=> Loading Checkpoint\n",
      "Test 06_19_2: (7888, 256, 256, 3)\t(7888, 256, 256)\t(3, 8708, 14955)\n",
      "=> Loading Checkpoint\n",
      "Test 06_19_4: (8400, 256, 256, 3)\t(8400, 256, 256)\t(3, 10356, 13522)\n",
      "=> Loading Checkpoint\n",
      "Test 06_19_6: (38720, 256, 256, 3)\t(38720, 256, 256)\t(3, 22639, 28233)\n",
      "=> Loading Checkpoint\n",
      "Test 07_31_1: (6936, 256, 256, 3)\t(6936, 256, 256)\t(3, 8731, 13076)\n",
      "=> Loading Checkpoint\n",
      "Test 07_31_3: (8400, 256, 256, 3)\t(8400, 256, 256)\t(3, 9721, 14412)\n",
      "=> Loading Checkpoint\n",
      "Test 07_31_5: (45684, 256, 256, 3)\t(45684, 256, 256)\t(3, 24192, 31214)\n",
      "=> Loading Checkpoint\n",
      "Test 08_25_2: (7888, 256, 256, 3)\t(7888, 256, 256)\t(3, 8708, 14955)\n",
      "=> Loading Checkpoint\n",
      "Test 08_25_4: (8400, 256, 256, 3)\t(8400, 256, 256)\t(3, 10356, 13522)\n",
      "=> Loading Checkpoint\n",
      "Test 08_25_6: (38720, 256, 256, 3)\t(38720, 256, 256)\t(3, 22639, 28233)\n",
      "=> Loading Checkpoint\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    \"06_03_1\",\n",
    "    \"06_03_3\",\n",
    "    \"06_03_5\",\n",
    "    \"06_19_2\",\n",
    "    \"06_19_4\",\n",
    "    \"06_19_6\",\n",
    "    \"07_31_1\",\n",
    "    \"07_31_3\",\n",
    "    \"07_31_5\",\n",
    "    \"08_25_2\",\n",
    "    \"08_25_4\",\n",
    "    \"08_25_6\",\n",
    "]\n",
    "gdf_pred_all = gpd.GeoDataFrame()\n",
    "for tif_num in layers:\n",
    "    test_tif, test_mask, tif_array, ndwi_array = icelake.data_preprocess_test(tif_num)\n",
    "    print(f\"Test {tif_num}: {test_tif.shape}\\t{test_mask.shape}\\t{tif_array.shape}\")\n",
    "    sliced_image_set = unet.run_unet([], [], test_tif, test_mask, print_tqdm=False, BATCH_SIZE=BATCH_SIZE)\n",
    "    lakes_pred = icelake.splice_image(ndwi_array, sliced_image_set)\n",
    "    del test_tif, test_mask, sliced_image_set\n",
    "\n",
    "    # lakes_pred post process in tif\n",
    "    tmp_mask = np.ones_like(lakes_pred)\n",
    "    tmp_mask[np.all(tif_array == 0, axis=0)] = 0\n",
    "    lakes_pred = lakes_pred & tmp_mask\n",
    "    # lakes_pred post process in gpkg\n",
    "\n",
    "    gdf_pred = icelake.array2gdf(lakes_pred, tif_num)\n",
    "    del lakes_pred, tif_array, tmp_mask\n",
    "\n",
    "    gdf_pred = icelake.post_process(gdf_pred, tif_num)\n",
    "    gdf_pred[\"tif_num\"] = tif_num\n",
    "    gdf_pred_all = pd.concat([gdf_pred_all, gdf_pred], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save testing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5316 lakes saved to path GPKG/lake_polygons_testing.gpkg\n"
     ]
    }
   ],
   "source": [
    "def get_image_name(tif_num):\n",
    "    tif_num = tif_num[:5]\n",
    "    if tif_num == '06_03':\n",
    "        return \"Greenland26X_22W_Sentinel2_2019-06-03_05.tif\"\n",
    "    elif tif_num == '06_19':\n",
    "        return \"Greenland26X_22W_Sentinel2_2019-06-19_20.tif\"\n",
    "    elif tif_num == '07_31':\n",
    "        return \"Greenland26X_22W_Sentinel2_2019-07-31_25.tif\"\n",
    "    elif tif_num == '08_25':\n",
    "        return \"Greenland26X_22W_Sentinel2_2019-08-25_29.tif\"\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "gdf_pred_all[\"region_num\"] = (gdf_pred_all[\"tif_num\"]).apply(lambda x: x[-1])\n",
    "gdf_pred_all[\"image\"] = gdf_pred_all[\"tif_num\"].apply(lambda x: get_image_name(x))\n",
    "gdf_pred_all = gdf_pred_all.drop(columns=[\"tif_num\"])\n",
    "\n",
    "gdf_pred_all.to_file(\"GPKG/lake_polygons_testing.gpkg\", layer=\"lakes\", driver='GPKG')\n",
    "print(f\"{len(gdf_pred_all)} lakes saved to path GPKG/lake_polygons_testing.gpkg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devgis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
